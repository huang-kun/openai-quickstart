{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e64d1c",
   "metadata": {},
   "source": [
    "\n",
    "# LangGraph Reflection 机制开发指南\n",
    "\n",
    "本指南详细介绍了如何在 **LangGraph** 中构建基于大语言模型（LLM）的 **Reflection（反思）** 机制。\n",
    "\n",
    "Reflection 是一种重要的模型能力，通过让模型观察其过去的步骤和外部环境反馈，评估自身行为的质量，并不断改进输出。在生成与反思的循环中，模型可以逐步优化内容，从而提升生成质量和用户满意度。\n",
    "\n",
    "Reflection 机制被广泛应用于生成任务中，例如文章写作、内容修改与反馈、以及智能助理等场景。通过引导 LLM 进行自我反思和用户反馈处理，开发者可以让模型在多轮交互中自动调整其生成的内容，达到高效、精准、结构完善的输出。\n",
    "\n",
    "\n",
    "\n",
    "在本指南中，我们会逐步演示如何搭建这一机制，包括从基础的环境配置到生成器和反思器的构建，再到如何使用 LangGraph 状态图实现生成-反思循环的完整流程。无论您是为文章生成、内容评估，还是其他复杂任务设计 LLM 代理，本指南都将为您提供详细的开发思路和实用的代码示例。\n",
    "\n",
    "![reflection](./images/reflection.png)\n",
    "\n",
    "通过本指南，您将学习如何：\n",
    "1. 设置开发环境并安装所需包；\n",
    "2. 定义和生成灵活结构的文章，不局限于传统的五段式；\n",
    "3. 通过反思机制批改生成内容，并提供详细反馈；\n",
    "4. 构建反思与生成的状态循环，使模型持续改进生成内容。\n",
    "\n",
    "本开发指南适合任何希望构建复杂 LLM 任务的开发者，特别是需要实现生成-反思流程、文章批改反馈、或其他高级交互任务的场景。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e06a35-b8fb-4475-ac56-eef76a78e3b2",
   "metadata": {},
   "source": [
    "## 1. 环境设置\n",
    "首先，安装所需的包并设置API密钥："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d045265-8b0b-42e7-9bec-9e18e62a8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langgraph langchain-ollama tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c166149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# 定义一个帮助函数来检查环境变量，如果不存在则提示用户输入\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"请输入您的 {var}\")\n",
    "\n",
    "# 设置 OpenAI 和 Langchain API 密钥\n",
    "# _set_if_undefined(\"OPENAI_API_KEY\")\n",
    "# _set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "# _set_if_undefined(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec8159-c202-4274-b4cb-eddfa337940a",
   "metadata": {},
   "source": [
    "## 2. LangSmith开发配置\n",
    "LangSmith能够帮助您快速发现问题并提高LangGraph项目的性能。通过LangSmith，您可以使用跟踪数据来调试、测试和监控基于LangGraph构建的LLM应用程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c231a35a-8f08-44d1-abda-5d0defd00dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 LangSmith 中添加追踪功能\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Reflection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75943d-3e39-4765-811a-2c9a47cf3722",
   "metadata": {},
   "source": [
    "## 3. 定义写作助手智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1905a06e-af05-4691-a6ed-014be2cfaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework for code generation\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a programmer for writing algorithm code based on the user's request.\"\n",
    "            \" Focus on clarity, structure, and quality to produce the code result.\"\n",
    "            \" If the user provides feedback or suggestions, revise and improve the coding to better align with their expectations.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "student_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.2, max_tokens=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0cec14-582a-4094-9a5b-9a0a2ae04a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = writer_prompt | student_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a374db97-f61e-44d0-9fb7-8be1d3368a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斐波那契数列是一个经典的数列，每个数都是前两个数的和。其基本形式是：F(0) = 0, F(1) = 1，然后 F(n) = F(n-1) + F(n-2) 对于 n >= 2。\n",
      "\n",
      "下面是用 Python 实现斐波那契数列的几种方式：\n",
      "\n",
      "### 方法一：递归实现\n",
      "\n",
      "```python\n",
      "def fibonacci_recursive(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n",
      "\n",
      "# 示例：计算前10个斐波那契数\n",
      "for i in range(10):\n",
      "    print(fibonacci_recursive(i), end=' ')\n",
      "```\n",
      "\n",
      "### 方法二：动态规划（迭代）\n",
      "\n",
      "```python\n",
      "def fibonacci_dynamic(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    \n",
      "    fib = [0] * (n + 1)\n",
      "    fib[0], fib[1] = 0, 1\n",
      "    \n",
      "    for i in range(2, n + 1):\n",
      "        fib[i] = fib[i - 1] + fib[i - 2]\n",
      "    \n",
      "    return fib[n]\n",
      "\n",
      "# 示例：计算前10个斐波那契数\n",
      "for i in range(10):\n",
      "    print(fibonacci_dynamic(i), end=' ')\n",
      "```\n",
      "\n",
      "### 方法三：优化的空间复杂度（仅使用常量空间）\n",
      "\n",
      "```python\n",
      "def fibonacci_optimized(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    \n",
      "    a, b = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        a, b = b, a + b\n",
      "    \n",
      "    return b\n",
      "\n",
      "# 示例：计算前10个斐波那契数\n",
      "for i in range(10):\n",
      "    print(fibonacci_optimized(i), end=' ')\n",
      "```\n",
      "\n",
      "### 方法四：生成器实现（使用 yield）\n",
      "\n",
      "```python\n",
      "def fibonacci_generator(n):\n",
      "    a, b = 0, 1\n",
      "    for _ in range(n):\n",
      "        yield a\n",
      "        a, b = b, a + b\n",
      "\n",
      "# 示例：生成前10个斐波那契数\n",
      "for value in fibonacci_generator(10):\n",
      "    print(value, end=' ')\n",
      "```\n",
      "\n",
      "你可以根据自己的需求选择上述方法中的任何一种。如果你需要更具体的功能或有其他要求，请告诉我！"
     ]
    }
   ],
   "source": [
    "article = \"\"\n",
    "\n",
    "topic = HumanMessage(\n",
    "    content=\"用python实现斐波那契数列\"\n",
    ")\n",
    "\n",
    "for chunk in writer.stream({\"messages\": [topic]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    article += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b8163-56b5-4b49-9dd3-aaf14f1566db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73fa01c1-9074-41ae-810b-450edc7261ea",
   "metadata": {},
   "source": [
    "----------\n",
    "## 4. 定义审阅老师智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7001a65a-88ca-4ab7-bc66-fa1df870f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a generalist master who can provide criticism and suggestions for any user-provided content (such as articles, code, reports, etc.). \"\n",
    "            \"The suggestions section contains detailed improvement steps or requirements to help users improve the quality of their content.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "teacher_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, max_tokens=8192)\n",
    "reflect = reflection_prompt | teacher_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0c878a-f333-4fb6-b879-e7636d1087ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的代码实现了斐波那契数列的多种方法，整体上非常清晰且易于理解。以下是一些具体的批评和建议，以帮助你进一步改进代码的质量和可读性。\n",
      "\n",
      "### 批评\n",
      "\n",
      "1. **递归实现的效率**：\n",
      "   - 递归方法在计算较大 `n` 时效率低下，容易导致栈溢出。可以考虑添加一个警告或说明，提醒用户在使用递归时的限制。\n",
      "\n",
      "2. **代码重复**：\n",
      "   - 在每个方法中，处理 `n <= 0` 和 `n == 1` 的逻辑重复出现，可以考虑将这些逻辑提取到一个单独的函数中，减少重复代码。\n",
      "\n",
      "3. **输出格式**：\n",
      "   - 在输出斐波那契数时，建议在每个数之间添加空格或换行符，以提高可读性。\n",
      "\n",
      "4. **函数文档**：\n",
      "   - 每个函数缺少文档字符串（docstring），可以添加说明，描述函数的功能、参数和返回值。\n",
      "\n",
      "### 建议\n",
      "\n",
      "1. **添加文档字符串**：\n",
      "   - 为每个函数添加文档字符串，说明其功能和参数。例如：\n",
      "   ```python\n",
      "   def fibonacci_recursive(n):\n",
      "       \"\"\"\n",
      "       计算第 n 个斐波那契数。\n",
      "       \n",
      "       参数:\n",
      "       n (int): 斐波那契数的索引。\n",
      "\n",
      "       返回:\n",
      "       int: 第 n 个斐波那契数。\n",
      "       \"\"\"\n",
      "   ```\n",
      "\n",
      "2. **优化递归实现**：\n",
      "   - 可以使用记忆化（memoization）来优化递归实现，避免重复计算。\n",
      "   ```python\n",
      "   def fibonacci_memoization(n, memo={}):\n",
      "       if n in memo:\n",
      "           return memo[n]\n",
      "       if n <= 0:\n",
      "           return 0\n",
      "       elif n == 1:\n",
      "           return 1\n",
      "       memo[n] = fibonacci_memoization(n - 1, memo) + fibonacci_memoization(n - 2, memo)\n",
      "       return memo[n]\n",
      "   ```\n",
      "\n",
      "3. **统一输出格式**：\n",
      "   - 可以创建一个辅助函数来统一输出格式，例如：\n",
      "   ```python\n",
      "   def print_fibonacci(n, method):\n",
      "       for i in range(n):\n",
      "           print(method(i), end=' ')\n",
      "       print()  # 换行\n",
      "   ```\n",
      "\n",
      "4. **增加异常处理**：\n",
      "   - 对于输入参数 `n`，可以添加类型检查和异常处理，确保用户输入有效的整数。\n",
      "   ```python\n",
      "   if not isinstance(n, int) or n < 0:\n",
      "       raise ValueError(\"n 必须是一个非负整数\")\n",
      "   ```\n",
      "\n",
      "5. **示例输出**：\n",
      "   - 在示例部分，可以添加注释说明输出的含义，帮助用户理解输出结果。\n",
      "\n",
      "### 改进后的代码示例\n",
      "\n",
      "```python\n",
      "def fibonacci_recursive(n):\n",
      "    \"\"\"\n",
      "    计算第 n 个斐波那契数。\n",
      "    \n",
      "    参数:\n",
      "    n (int): 斐波那契数的索引。\n",
      "\n",
      "    返回:\n",
      "    int: 第 n 个斐波那契数。\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"n 必须是一个非负整数\")\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n",
      "\n",
      "# 其他方法同样添加文档字符串和异常处理\n",
      "\n",
      "def print_fibonacci(n, method):\n",
      "    for i in range(n):\n",
      "        print(method(i), end=' ')\n",
      "    print()  # 换行\n",
      "\n",
      "# 示例：计算前10个斐波那契数\n",
      "print_fibonacci(10, fibonacci_recursive)\n",
      "```\n",
      "\n",
      "通过这些改进，你的代码将更加健壮、可读和易于维护。希望这些建议对你有所帮助！如果你有其他问题或需要进一步的帮助，请告诉我！"
     ]
    }
   ],
   "source": [
    "reflection = \"\"\n",
    "\n",
    "# 将主题（topic）和生成的文章（article）作为输入发送给反思智能体\n",
    "for chunk in reflect.stream({\"messages\": [topic, HumanMessage(content=article)]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c616014-c9c9-4d46-be9f-87485ee750eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ad3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated  # 用于类型注解\n",
    "from langgraph.graph import END, StateGraph, START  # 导入状态图的相关常量和类\n",
    "from langgraph.graph.message import add_messages  # 用于在状态中处理消息\n",
    "from langgraph.checkpoint.memory import MemorySaver  # 内存保存机制，用于保存检查点\n",
    "from typing_extensions import TypedDict  # 用于定义带有键值对的字典类型\n",
    "\n",
    "# 定义状态类，使用TypedDict以保存消息\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # 使用注解确保消息列表使用add_messages方法处理\n",
    "\n",
    "# 异步生成节点函数：生成内容（如作文）\n",
    "# 输入状态，输出包含新生成消息的状态\n",
    "async def generation_node(state: State) -> State:\n",
    "    # 调用生成器(writer)，并将消息存储到新的状态中返回\n",
    "    return {\"messages\": [await writer.ainvoke(state['messages'])]}\n",
    "\n",
    "# 异步反思节点函数：对生成的内容进行反思和反馈\n",
    "# 输入状态，输出带有反思反馈的状态\n",
    "async def reflection_node(state: State) -> State:\n",
    "    # 创建一个消息类型映射，ai消息映射为HumanMessage，human消息映射为AIMessage\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    \n",
    "    # 处理消息，保持用户的原始请求（第一个消息），转换其余消息的类型\n",
    "    translated = [state['messages'][0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in state['messages'][1:]\n",
    "    ]\n",
    "    \n",
    "    # 调用反思器(reflect)，将转换后的消息传入，获取反思结果\n",
    "    res = await reflect.ainvoke(translated)\n",
    "    \n",
    "    # 返回新的状态，其中包含反思后的消息\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef78c4fb-2db3-45c0-9784-b73de4e7ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUND = 6\n",
    "\n",
    "# 定义条件函数，决定是否继续反思过程\n",
    "# 如果消息数量超过6条，则终止流程\n",
    "def should_continue(state: State):\n",
    "    if len(state[\"messages\"]) > MAX_ROUND:\n",
    "        return END  # 达到条件时，流程结束\n",
    "    return \"reflect\"  # 否则继续进入反思节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e188e5e-2327-4c78-927e-5f778fdca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建状态图，传入初始状态结构\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# 在状态图中添加\"writer\"节点，节点负责生成内容\n",
    "builder.add_node(\"writer\", generation_node)\n",
    "\n",
    "# 在状态图中添加\"reflect\"节点，节点负责生成反思反馈\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "\n",
    "# 定义起始状态到\"writer\"节点的边，从起点开始调用生成器\n",
    "builder.add_edge(START, \"writer\")\n",
    "\n",
    "\n",
    "# 在\"writer\"节点和\"reflect\"节点之间添加条件边\n",
    "# 判断是否需要继续反思，或者结束\n",
    "builder.add_conditional_edges(\"writer\", should_continue)\n",
    "\n",
    "# 添加从\"reflect\"节点回到\"writer\"节点的边，进行反复的生成-反思循环\n",
    "builder.add_edge(\"reflect\", \"writer\")\n",
    "\n",
    "# 创建内存保存机制，允许在流程中保存中间状态和检查点\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 编译状态图，使用检查点机制\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce358b-9b0d-4297-94e2-6ed8ab7e4dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28225cf-55bc-4cc3-8fc7-45064d224782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOQAAAD5CAIAAAC4fQ6fAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XlAE9e+B/CThZCdsIYlbIoLKig2bqhYRItVpFqtWsUu7tZqbautFu2rt8utW60bttXrUml91WvdF9zqAgoobgiIorLvWQnZk3l/jI/rrQEJJjkzyfn8JeNk5hfyZXJm5sw5FAzDAIKQARV2AQjSXiisCGmgsCKkgcKKkAYKK0IaKKwIadBhF2AXNaUatdKkbjKZjJhea4ZdTrswmFR3FpXDp7P5NJ9Ad9jlEBHFaa6zYhhWlNv0OF9VWqAO6c6mu1HYPJrAj6HXkCOsFCpQNBqalUYWh1b9WBvei9M5iiPqyoZdF4E4SVhv/SW79ZcsNJLTKYob3osDu5yX1SQzPLnXXF+pk9cZBo31DurMgl0RIZA+rBUP1Bl7arv35w8e602hUmCXY2M1pZprxySeQkb8JD/YtcBH7rDeviSveKAeMVXI4tBg12JHFQ/Vp3bWvv1ZMM/TDXYtMJE4rAXZCmmNfuh4X9iFOIJOY9q3pmLKkmCmU/9Zto2sYc083Gg0ml+d6Fpfjnu+Lk2eE+gpZMAuBA5SXmctylVq1SZXSyoAIOWL0H1rymFXAQ35wlpfoa18qB4xVQi7EAhoNMqkT0UZe2thFwIH+cJ65XBjz4EesKuAxieQSQGgOK8JdiEQkCyspYXNDHdqoGtfd4wd63P1WCPsKiAgWViLbzTFJnvDrgIyroDeK9ajMEcBuxBHI1NYFRJDXZnW299B981VKtX9+/c7/PKamprq6mqbVvQfAeHM4hsqO22csMgU1if5zeFRjruVOmXKlCNHjnTstZWVlcnJyYWFhbYu6ilRF3ZdudagI0e3B1shU1jryrURfbgO251er+/YCzEMMxqN9r6A3WMgv6yo2a67IBoyhbWqRMP3ssv9xt27d48ePXrIkCEzZ87Mzc0FACQlJUml0gMHDojF4qSkJDy7W7duTU5OHjBgwJgxY9LS0kwmE/7y1atXv/baa5cvXx4/frxYLD516tTEiRMBAMuWLROLxV999ZU9anZnUqV1BntsmbDI1J9V3WTi8G1fcG5u7pYtW0aNGhUbG3v16lW1Wg0AWLNmzYcffvjKK69MmzaNwWAAAGg0Wk5OTlxcnEgkKi4u3rlzJ5/PT0lJwTeiUqnS0tKWLVum0WgGDRpEpVJXrFgxb948sVjs5eVl85oBABw+vaFaZ48tExZpwtqsNLJ5drktjp8GTZo0KTo6evTo0fjCHj160Ol0Hx+fPn364EtoNNqePXsolKcduyorKy9cuNASVr1ev2LFil69euE/du/eHQAQFhbW8nKb43jQS12sGUCasJpNGItrl7AOGTKEz+evXLly6dKlQ4YMaWNNqVS6ffv27OxspVIJAODxeC3/xWQyW5LqGDQ6oNGcrUtk20jTZuXw6dK6Dp7xtM3Hx2fnzp2hoaGLFy+eOXNmfX29xdUkEsm0adNyc3Pnz5+/efPmyMjIljYrAIDNdnSXfpXcxGCR5uOzCdK8WyqN4s6ialSmdqxrtbCwsE2bNm3btq2kpOTZ86Fnz+gPHjwolUrT0tISExN79uzp7+9vj0rar1lptEcLnshIE1YAQEg3trrJaI8t41ep+vXrN3To0JYbASwWq7HxP3c15XK5p6dnS0blcnkbF6eYTCYAoKGhwR7V4kwmTODnWn2xyfSnKfBjlNxWeQfY+A5WQUHB559/PmnSJDabffXq1R49euDLY2JiTp8+vXv3bj6fHx0dLRaL9+/fv23btt69e1+4cCErK8tsNsvlcoFA8Pw2hUJhUFBQeno6i8VSKBRTpkxxd7dx2YXXlJOXBNt2mwRHpiNreC/Ok3u2P/9lMBjh4eG7du3asmVLTEzMypUr8eWLFi0Si8U7duzYtWtXRUXF8OHDZ82adeDAgdTUVIPBsHv37rCwsD/++MPiNikUynfffcfhcNatW3fs2DGpVGrbmuvLtRwB3dWaASR7UuDY9ur4Sb5cD9f6+nve7YsyQKH0GWbhoO7ESPanGRHNzT4pHfF2qz2vly1blp2d/fxyoVBYV1f3/HIPD48OdwBov8zMzBUrVjy/HMMwDMOoVAvfbydOnOBwLHeEMJuxrKOSBT9E2KFSQiPZkRUAsPfbsrFzAgS+lp9DkkgkOp2F+zoGg8HNzcLxmEqlOuC8XqvVWmwJmM1ms9lMp1s4ZPj7+1sMMQAg80gjh0+Life0Q6WERr6wPrmnqnyocZGHWp+naTadTa9NnhsEuxAIyHSChQvvxaUzqDfO2viUhSz+WFfhsgNekC+sAIBBY7xry7T3rrlcV/lDWyuHTfR12aEuyNcMaHHp3/Xege69Yl3l4cFDaVVD3vDxDXLdAQZJeWTFDZvoV1euzTrq/I/ONSuMu7560jde4MpJJfeRFXfnsjzvnGxQkndkfz7sWmxPrzVfPd6olBiHT/bjCkh2ndHmSB9WvEvHteMSWb2+Sx9eeBTHw9sZmnSVD9U1T7Q3L8hik3yihrhKU6dtzhBWnLRWX5CteJLfTGdQRV1Y7iwqx4PO83QzmcjxBjETaJIZmpVGQAH3shR+wcyIPpyowa51j6ptzhPWFpIaXV25ViU3NSuMNBqlSW7jjlolJSW+vr4eHjY+2rF5NDqDwuHT+V70kO4cBpPEpxN24oRhtbfFixdPmDBh6NChsAtxOejPFyENFFaENFBYrSYUCi12PUHsDYXVanV1dUajXZ6uQdqGwmo1FovVMnoA4kgorFbTaDToEgoUKKxW4/P5rXWLRuwK/dKtplQqzWbXGmuSIFBYrRYYGGjxCRnE3lBYrVZdXW0wuNZYkwSBwmo1fARMxPFQWK3W4RGxkZeEwoqQBgqr1QIDA9HtVihQWK1WXV2NbrdCgcKKkAYKq9XYbDa6gwUF+qVbTa1WoztYUKCwWs3Pz49Gs8tUHEjbUFitVl9f/+zUF4jDoLAipIHCajXU+RoWFFaroc7XsKCwIqSBwoqQBgqr1QICAlDfAChQWK1WU1OD+gZAgcKKkAYKK0IaKKxWQ9dZYUFhtRq6zgoLCitCGiisVvP29ka9rqBAYbWaRCJBva6gQGFFSAOF1WqoDQALCqvVUBsAFhRWqwUEBKCDKxQorFarqalBB1coUFithh4YhAVN2tZeI0eOZDKZAACpVMrhcNzd3fERBQ8ePAi7NFeB+mW2l6en5+PHj/F/63Q6AIDZbJ4+fTrsulwIaga014QJE/CjaQuRSPT222/Dq8jloLC21/jx40Ui0bNL4uLi/P394VXkclBY24vBYIwfP77l4BoYGIjaAA6GwmqFN998Ez+4Yhg2bNgwoVAIuyLXgsJqBQaDMXbsWBqNFhgYmJKSArscl+NsVwO0apOkWq/T2muUvwFRyRfCC6Kjo5sbuI8bmu2xCwrAeF5unn4MGh09j/BfnOc6K2bGMvbWlt/XBHVhmwwkflPubFpjlZbuRonsz4seKoBdDoE4SVj1OvPBTZUx8d5BXTiwa7GZrMN1whBGTLwn7EKIwknarAc3VsYmC50pqQCAweOEdeX6/CwF7EKIwhnCev+GMrAz28vfvR3rksygsX5FOUqTyRm+/V6eM4S1vkLH5DrbmSKOSqPotGZFI5p9EzhJWPUaM8/LaWf+9Q1iKiUorMBZwqo1Y847IYVOYwKoFQCAk4QVcREorAhpoLAipIHCipAGCitCGiisCGmgsCKkgcKKkAYKK0IaKKwIaaCwIqSBwmqFx49Lkt+Iz8y6iP+oUqkePLwPuygXgsJqBTqdzuXy6LSn3RFnzZly6tQR2EW5EOfsBmpzGIZRKJSQkLDffzvaslCv17/M1mxXnatwuSMrhmFjk19dt/6bliXLUxcrFHL83xJJ4/AR/U5nHFMo5PEJ4j/27/3muxWvjxny0cezT2cci08QxyeIb+TlAACmTE2SyaSHjxyITxBPmZqEv1yr1W7Zun78hJFjxsbNmz/9wl9n8OUXL52LTxBnZl5c+NHMkYkDDx85AOOtk57LHVkpFErs4GFXr102m81UKrWurjYnJ+t0xrHJk6YDAC5dPk+j0WJjh2FmMwAgPf1fb7zx1vp1P9FoNIGH55zZC3/Zvhnfzlf/s+azzz/s0/uVtyZOc2Mw8HHaUld8XFtbPW3q+wKB1+3bN77+5gutVjP69Tfwl2zcvHrWjAUz3p8fFtoJ5q+AtFwurACAV+NGnDlzorAwv1ev3qczjmEYdvzEof8P67m+ffvzeXz8WNujR9SsmQtaXtg7um/Lv7t360Gn0729faKi+uBLLl+5cDf/1r7fjvn4+AIARiSM0mjUB//c1xLW8eMmJyYmOfztOg9XDKtYPJDL5WZmXezZMzoj49iY0eNOnT56+3ZecHBofv7tz5Z+2bJm377927/Z7OxMo9E4NSW5ZYnJZOJwuB3bGvI8Vwyrm5vboEFxWVcv9e8fW99Q9+47cxQK+YmTh3r0iMbbAC1rMpms9m9WJpN4e/v8sO6nZxfS6P/5DbNZbBu9AxflimHFWwJnz57cvmNL7KA4X1+/sWMnrFj5SVnZE7wN0P7tPDtECI/Hl8tlQmHA34ZxRWzF5a4G4MTigRwO5/79grFjJwAA+okH+vkKH5YUx786sv0bYTFZEkljy499+/Y3mUxHj/27ZYlGo7F14S7NRY+sDAZj0KC4wsJ88SsD8EsESUlv/mtn2rNtgBeKioo5f+H07/t283j8nj2iR44Yfez4nz/9vLGmtrprl+4lJQ8ys/7avfPf+EwEyMtz0bDiLYGIzl1bLs6/Piq5oOCuVW2AuXMWSaWNe9N3CDw8P/jgk06dItau3rp9x+YLFzKOH/9TJApJHjuRTnfd37DNOcPAbCd31oT25Id0d6qBrlpc2Ffde6hHWE/nfHdWcdE2K0JGKKwIaaCwIqSBwoqQBgorQhoorAhpoLAipIHCipAGCitCGiisCGmgsCKkgcKKkAYKK0IazhBWjsANOO9T+GwenebmvG/PGk4RVh61oUILuwp7KS1Q+QSh52SAk4Q1pDtbJe3g4CgEJ63VBkawWBwa7EIIwRnC6hfMDOjEzDxcB7sQGzMZsYv7a+Pf8oVdCFE4w5MCuDtXFE8KmkO7c32CmG7uJP4jpFCAolHfJDPknGx498swhao+MDAQdlGE4DxhBQBUlahP/HHXg+2nUdoxrFqt1o1Op9nt4SqupxuNBgI7MweM8gYAKBSKUaNG7d+/Pzg42E57JAunepxNpn1MFd6bPGuW/XZRVFT0+eefh4aGbt682X57eZaHh8elS5cyMzNRWEn8dfmsvLy8mpqakJCQWfZMKgBg//79VVVVDx8+vHbtml139CwGgzF8+HAAwLhx4xy5X6JxhrDeuXPn559/DggI4POteJC6A4qKivLy8igUSmNjY3p6ul33ZdHhw4dv3LgBADAajY7fO3SkD6tOp8Mw7JdffnHAvtLT06urq/F/P3r0KDMz0wE7/ZuFCxcCALZu3ZqRkeH4vcNF4rA2NDQMGDCATqf36dPHAbsrKiq6fft2y4+wDq64jz766ObNm652iCVxWK9fv56VlUWjOeiCeXp6ek1NzbNLHjx4kJWV5Zi9P2/58uUAgIyMjCNHXGVeA1KG9Z///CcAYPTo0Y4cnCcvL49KffrrMpvNAAClUrl7926HFWDRmDFj7ty5A/FvxqEwslmwYEF2djbEAlJTUzMzMyEW8LyamhoMw86fPw+7EPsi05E1Pz8fAPDNN98MGDAAYhkqlQo/shKHv78/AODatWt79+6FXYsdkSasu3fvvnv3LgBAIBDAroWgUlNTo6KiAAAPHz6EXYtdkCasHA5n2rRpsKsA+CjvsEtoFX5hJCsra+3atbBrsT2ih1Uul+/ZswcA8NZbb8Gu5SmDwQC7hBd47733goODTSaTVutU3XwJHVaTyTRhwoRJkybBLuS/BAYGEvngipsyZQqNRsvLyzt+/DjsWmyGuGEtLS3V6XTnz59nsayYMsUBlEqlyWSCXUW7DB48+Pr16w8ePIBdiG0QNKzbtm2Ty+VsNhHn4pFKpSQafH3VqlUCgUAmkzU2NrZjdUIjYliVSqWbm5tjbqJ2gEAg4PF4sKuwgp+fn4eHx7Rp08rKymDX8lIIF9bLly8zGAx79/R7GVVVVX5+frCrsA6VSs3IyCgpKYFdyEshVlhnz57dtWtXIs/FI5fLKysrfXx8YBfSEQkJCQCAzz77DHYhHUSgsKpUqvnz5+M3YwiruLh45EgrJnYjoMTExLS0NNhVdARRThSKi4uDgoL69u3bjnVhysjI6N27N+wqXkpCQgJ+o8tkMjmsz5pNEOLIumDBAplMxuVy27EuZMXFxYmJibCreFl4m3vq1KnkumsA/+nWqqoqLpfr4eEBt4z2+PPPP4uKilJTU2EXYjMbNmz4+OOPYVfRXpDDWl5ebjKZwsPDIdbQfu+8887q1asDAgJgF2JL5eXlQUFBpGgPwGwG3L9/f/ny5WRJ6r59+6Kjo50sqQAAX1/fuLg42FW0D8S+tIcPH9ZqtRALaD+DwTB58mTYVdiLVCo9ePAg7CpeDH6blRQWLVo0efLkwYMHwy7EpcFpBhiNxhEjRkDZdQecOnUqMjLS6ZOampqanZ0Nu4q2wAnroUOH3n77bSi7tlZhYeHvv/8+f/582IXY3cqVKy9evAi7iragZkBbMAxLSko6ceIE7EIQAOfIqtfri4uLHb/fDnjjjTccM9YLcWzbtg12Ca2CENb79+9///33jt+vtWbNmrVq1aqgoCDYhTiUTCY7ePAg7CosgxDW5ubmLl26OH6/Vvnxxx+nTZsWExMDuxBHW7RoUVhYGOwqLENtVgtWrVoVExOTnJwMuxDkv8BpszY1NTl+v+20dOnSyMhIV07qxo0b8/LyYFdhAYSw1tfXp6SkOH6/7bF69erZs2cT7XlaB2MwGLdu3YJdhQUQ+rOKRKKAgACDwUC0B5rXrFkTGhratWtX2IVAlpKSolKpYFdhAWqzPrVx40Z/f//JkyfDLgRpFZw7WA0NDTKZDMquLfriiy/Cw8NRUnElJSXLli2DXYUFcMKam5u7YcMGKLt+3qpVq4YNG+bKZ1R/YzQaKyoqYFdhAZywxsTEEGR88RkzZrz22mtO8KSKDXXq1ImY47q5dJt1woQJX375JdkfAHQd0J4UKCkpSUpKSkhI6Nev35tvvungvRuNxrlz565fvx4l9XmVlZVfffUV7CoscPSlq7i4OLVabTabKRQKhULBezY5+K5mXV1dcnLymTNnSPGUouOp1Wpi9jRy9JE1NjYWwzAqlYonFb8E7chh1+/du7d69eqcnByU1NYEBgZ++umnsKuwAEKbdeLEiaWlpS0/BgUFbd++3TGjR2VlZf3yyy/46MQI6UBos37//ffPzknu5+fnmKQeOHDg2rVrKKkvVFlZ+fXXX8OuwgIIYY2IiJg7dy4+z6rZbMaHsrG3LVu2PHr0aMmSJQ7YF9mp1erCwkLYVVgA52rAmDFjEhMTqVQqj8dzQIP1yy+/5HA4xLwrQ0BBQUHEHGmwXW1Wo8GsUdl+5qelS5dKJJJt27a5u7vbfOMtUlNTExIS8CnQn8IAz4soI9Ih7feCsBblKu9eUUhr9Syu7YeXwTCs5ZqAnZjNZgzD/jY2jk+ge2WJuktvbmyyjz3eF0mlpKQUFBTgU37iqcA/HeL0bW3rAJN7RtpYbRj6pj/Pi1h9+V6eQW+W1enSvy2d8lkIz9PZ3l3HLFiwIDU1ValUtsQUAECowZ1abbPmnJYqGoxDxwudL6kAADcG1S+YNWVZ599Xl+s05Jh6xd4GDRr0t2fj3N3dCdUP3XJYZfX6xirdwCSSjZzfAfFTArKOSWBXQRTTp0/Hr9LgRCIRcebKazWsjVU6DLNva5IgBL6MJ/nNsKsgiiFDhnTr1g3/N41GI1RSWw2rSmHyDSbuLBQ2xOLSvQPdNU2oJfBUSkoKPv2YSCSaOHEi7HL+i+WwGnRmg5ZYs5TbT2OVlkKI0eoJYfDgwd26daNSqUQ7rBJoAgykY/Rac+VDtUpuVDeZMDNobrJBl/bhUR95m+94m189t6/uJTdFAYDOoHL4NDaP7uFDD4p4qSkjUVjJ6u4VeXGeSlKt8wvnGY0YzY1OY9AxzAYfKJMTPCA2WKWxRZUYhjVhDXVGk0FPo4HG8prwXpwufbmdenVkshMUVvLJOy+7dlzi39WD7Svw7UasWZjb5hnq3VSvvpOluXZCGjfeJ7irdQdaFFYyqSvTnvmt3p3P6jkyzN43/+yBRqcKArkAAKZAd/FPqVCkei3Fisuj6MyCNAqyFad+rQ/sFeDX2YuMSX0Wi+8e3DvAQGVvT32iUbX3UgwKKzmU3FHdy1aHiYNobs7zkXE8WWH9An/9plSvbVdeneedO7FbF2W555qE3ZzwhqKbO73bsLBdX5Vpml+cVxRWoqsq0RRkNwf2cMKktug0IOj378tfuBoKK6Fp1cbMY1JRb2ebKe5v3Jh0/+4+Z/fVt70aCiuhXTkkYXBf6kI6WXC82NWPddWP2rq6i8JKXEqJoey+xlPEb8e6zsC3k9flw41trABxRJYHixbPen3MkCVLP1Ao5PEJ4iNH//0yG6ytramprbZdgfDd/EsujPCCXYUFjZKKJSsH3Lp7xrabZQuYdCajvLjVTnBwwmowGFZ8+QmGYf/z5er335v38husqq6cmpJcXEzEZzI7rChHyfEi0w2ql0d1Yzy42WpY7XIH64UPV5WWPa6rq12Z+l3PntEAAIVC/pJ7NBmNTjbCXMUDNc/bnUp3rXYaz4/95HqrX482C+v7MyeFh3UOC+v856H/1em0B/44zeVyb92+sX3HlkePHnh6esX06Tdr5gJvb59f9+7YtfsnAMCHi2bw+R5HDp1/fms1tdVpaT/k3cxhMNy7duk+Y8YH3bv1wP8rP//2nl9/KSzKBwD07v3K++/N4/H4774/EQCw6h/LVgGQmJi07DMijitmlcoSDce3I7092qPkcd7Js2nVtQ94XK+IcPHrI+fzeT5V1cVbdsyeOX3DyTNp1bUPPAUBY177sFfk09ndVc2yIyc3FNy/7EZ37xz+ip0Kc3OnC/xZdeVaYYiF7tS2/MO9fv3a/eKC777Z8PU/1nO53LybuZ99/mFYaKcln66cNDHl7t2bnyyZp9Vq418d+d67cwEAc2YvXL7sH89vRyJpXLhohrJJ8eGCJXPnLDIYDB8tnvXkySMAwPUb2R9/OrepSTlv7uI5sxeZTSaT0ejt5ZP6xTcAgPffm7fpxx0pU2fY8E3BUl+up9nnsPrw0fXtvy4S+oVPGpcaFzv1cemtn3Yt0Ou1AACDQZf+R2pc7JT5M7Z5Cvx/P7CyuVkOADAY9T/vXlhQdCkuduqYxA+lMjueG+g05iap5Y6OtmwG0Oj0lanfsVhPm1mbt6wdm/TmooVPh0sQiwe++/7E6zeuDR0Sj3/7947u26OHheFY9qbv8BR4rV+7jU6nAwBGjhid8s644ycPLVywZMvWdf7+gZs37WQwGACAcW887SDctUt3AEBISFhUVB8bviOImpVGgYddHhM/fGL9QPH48UlPB6fpGjFg7abJxSXZXoIAAMC4MZ/2iRoJABg98oMft737qPRWdM/4rOwDNbUP57y7uWtEfwBAWHDUmk32GtKexqA3K+wf1sjIXi1Jra2tKSt7UlVVcfzEoWfXqa9/cX/enJys+oa60UlDW5YYDIaG+rqa2ury8tJZMxfgSXVuOo2Z7m77MwqprKau4UmjtCL7xuFnl8sVdXhYGW5PP0FPQQAAQNnUAAC4V3QpQBiBJxUAQKXacbAFOoPWrLR/WFnM/5y6ymQSAMC778yJGzr82XW8vHxeuB2pTDJo0NA5sxY+u5DD4dbX1wIA/HyFNqyZsDAzBoDtTxmbVBIAwMj4WdE94p9dzuP5SKVVzy6h09wAAGazCQAgV9QGBXSzeTGWYfgTBhbYqz8rl8sDAOh02pAQqycC5fH4CoX8+Rc2N6vwKNuuTOJicWlGnYnBsvGgDSwmD2+b+vla8blwOZ6qZgfNrmMymLgCy9+c9royIhKFCIX+p04f1Wie3kAzGo0Gg8HiynS6GwCgqUmJ/9i3b/979+4UPyhqWQHfSHBwqK+vX8aZ4y2TZ2AYZjabAQDu7kwAgKSxwU5vx/E4fLpRb/tnbn19QgQe/tdvHtPpn34uJpPRaLT8ubQICuhWUVVY31Bm83qeZzIY2XzLx1CaxdHjqx5pTEbgH2bFFekjRw94CryGDRuB/0ihUITCgJMnj1y9dhnDQGFh/qbNawxGA35GVV1TdfbsyTGjx/n6CvHBr8+dO3nz1nUul9eta2SnTl3Onjt59uxJk8lUUVn22287L105Pzw+kUKheHp6Hz12MCcn02AwFD8o2rxlrTvDvXPnLhwO5+zZk/kFt9lsTl5eTtcukfjJWXsUXJVFDxXQGcS6oimX6BVSwOLbeMg6CoXiKQjIzTtaeP8KBrCyivxDx9ebTPrQ4KimJkn2jUMx0Ym+PiEAAJPJcOHynm5dBoYGRwl9w6/mHrx976zZbJJIqy5c+VUirYruOTxA2Nm25QEANDJ1pJjDsZRXO35CQ4fE//PbH93oblvT1v+avkMoDIiO7tvayqmp34pEIRlnjgMAggJFWzbt7Nkz+rffd25NWy9XyEYkvI6vNiJh1Nf/WIdh2LafNqT/9i+BwDNIFIJ/BitWfMdmc7ZsXXc645hMJrXf+3KM4Ai2qsEuo29E9Xh1RsoPNJrb0ZMbzl3c6enp3ynsBXM6+HiLZr+zUcD3y7iw/ezFnYHCLm2v32F6jbFZqvMVWR6zwvIogrkZUr0W9H6ViDembe6PtY9TlocyOYQbTvDnZY8jYkU0N8IVZj+ScoW3l/HVtyx33kUPDBJXj4F8SaNGENDqfawzf+24fHXf88tFAd0ra+5bfMn8PjuaAAACwElEQVTC2TuEfjYbGPDk2bSruQefX85i8jTaJosvWTxvt493cGsbNOkNETG81v4XhZW4XkkQpH9X0UZYhwyYJO4z+vnlFEqrw+568G35xMGwwdMGisc9vxzDQGt9Q9ooQCXRUDGjqPWBMFBYiYvNo3cTcxvLFd4hlmdBYrP5bDbM3q4ctgeHbbMZmhofS0fPaOtviVinwMjfDB3nbVCpYVfhCM3S5vCeLL9WTq1wKKyERqVRE6b4lt2oase6JKZV6aWlsmETfNteDYWV6HyD3PsnCiru1MIuxI5KrlalLA954WoorCQQ2Z8fP8GrKr8GdiG2p23S3zvzZP7aThTqi8eYQWElB1EX1qDRgpKsCp1aD7sWm2lqUDU8bFjwQ+d29txFYSWNTr24kz4JUpRLaosb7NFtwJGaGtWl16s8uPrpqSHtH7cLXboiE76X26SPRUU5yitHKj2EHKYHi+/Lbs8XKEHo1QZlgxqYDBTMOGam0DfIup4PKKzkEzmAHzmAX3xD+eBWc+GFet9QrtGA0dxobkwGAZ+aNBtNJoPJpDfR3IBaru8cxYmI4Qd17shTuyisZNVNzO8m5gMAqkrUzUpTs9JoMmDtHz7SQSiYG4PK8WBy+DS+N90n8KVmVUFhJb2XHKifRCyHlcGkmFt5tMD5+IqYZsJ9eSIWWL4awPN0ayizyQQIRNesNEpqdGw03TAZWA6rX7A7yccBby9ZnbZztL3GkkBsq9Uja1AE8/JBZ77Fhzv/W23c+Bc/cIsQQasdHwEABdcUD2+reg/z9hQy7DQ6CCwqhUFRrz/3W82sb8OYbHSWSQ5thRUA8KSg+fYlee0TLY3uPM0Cv2B3eYOhczRnyDgfsk974lJeENYWOo3zTOWKYRiTjc6oyKe9YUUQ6JyqJYo4NxRWhDRQWBHSQGFFSAOFFSENFFaENP4PPhxplQK5S34AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化图\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(\n",
    "            graph.get_graph(xray=True).draw_mermaid_png()\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error generating graph: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16cf4e0-abc5-4956-990c-09f78254b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 定义装饰器，记录函数调用次数\n",
    "def track_steps(func):\n",
    "    step_counter = {'count': 0}  # 用于记录调用次数\n",
    "    \n",
    "    def wrapper(event, *args, **kwargs):\n",
    "        # 增加调用次数\n",
    "        step_counter['count'] += 1\n",
    "        # 在函数调用之前打印 step\n",
    "        display(Markdown(f\"## Round {step_counter['count']}\"))\n",
    "        # 调用原始函数\n",
    "        return func(event, *args, **kwargs)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# 使用装饰器装饰 pretty_print_event_markdown 函数\n",
    "@track_steps\n",
    "def pretty_print_event_markdown(event):\n",
    "    # 如果是生成写作部分\n",
    "    if 'writer' in event:\n",
    "        generate_md = \"#### 生成内容:\\n\"\n",
    "        for message in event['writer']['messages']:\n",
    "            generate_md += f\"- {message.content}\\n\"\n",
    "        display(Markdown(generate_md))\n",
    "    \n",
    "    # 如果是反思评论部分\n",
    "    if 'reflect' in event:\n",
    "        reflect_md = \"#### 评论反思:\\n\"\n",
    "        for message in event['reflect']['messages']:\n",
    "            reflect_md += f\"- {message.content}\\n\"\n",
    "        display(Markdown(reflect_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64544540-9594-4812-a66a-c019284bdf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2954d151-db4c-46cf-97db-1ce5bf9fc7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Round 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 生成内容:\n",
       "- LRU（Least Recently Used）缓存是一种常见的缓存淘汰算法，其目标是为了保持最近使用的数据在缓存中。在这个算法中，当缓存满时，它会淘汰最久未使用的数据。我们可以使用 `OrderedDict` 来实现这一算法，因为它保留了插入顺序，可以高效地移动访问过的元素到尾部。\n",
       "\n",
       "下面是一个使用Python实现LRU缓存的示例代码：\n",
       "\n",
       "```python\n",
       "from collections import OrderedDict\n",
       "\n",
       "class LRUCache:\n",
       "    def __init__(self, capacity: int):\n",
       "        self.cache = OrderedDict()\n",
       "        self.capacity = capacity\n",
       "\n",
       "    def get(self, key: int) -> int:\n",
       "        if key not in self.cache:\n",
       "            return -1\n",
       "        else:\n",
       "            # Move the accessed item to the end of the OrderedDict (indicating it was recently used)\n",
       "            self.cache.move_to_end(key)\n",
       "            return self.cache[key]\n",
       "\n",
       "    def put(self, key: int, value: int) -> None:\n",
       "        if key in self.cache:\n",
       "            # Update the value and mark as recently used\n",
       "            self.cache.move_to_end(key)\n",
       "        self.cache[key] = value\n",
       "        \n",
       "        # If the cache exceeds the capacity, remove the first (oldest) item\n",
       "        if len(self.cache) > self.capacity:\n",
       "            self.cache.popitem(last=False)\n",
       "\n",
       "# 示例代码使用\n",
       "lru_cache = LRUCache(2)  # 创建一个容量为 2 的 LRUCache\n",
       "lru_cache.put(1, 1)      # 缓存是 {1=1}\n",
       "lru_cache.put(2, 2)      # 缓存是 {1=1, 2=2}\n",
       "print(lru_cache.get(1))  # 返回 1\n",
       "lru_cache.put(3, 3)      # 取消键 2，缓存是 {1=1, 3=3}\n",
       "print(lru_cache.get(2))  # 返回 -1 (未找到)\n",
       "lru_cache.put(4, 4)      # 取消键 1，缓存是 {3=3, 4=4}\n",
       "print(lru_cache.get(1))  # 返回 -1 (未找到)\n",
       "print(lru_cache.get(3))  # 返回 3\n",
       "print(lru_cache.get(4))  # 返回 4\n",
       "```\n",
       "\n",
       "### 代码说明：\n",
       "\n",
       "1. **`__init__`**: 初始化一个容量为`capacity`的LRU缓存，使用`OrderedDict`保存缓存内容。\n",
       "2. **`get`**: 返回缓存中`key`对应的值。若`key`不存在于缓存中，返回`-1`。同时，访问过的`key`会被移动到`OrderedDict`的尾部，表示它被最近使用过。\n",
       "3. **`put`**: 将`key`和`value`添加到缓存中。如果`key`已经存在，则更新其值并移动它到尾部。如果缓存的项数超过容量，移除最先插入的项（即`OrderedDict`的首项）。\n",
       "\n",
       "你可以根据需要进行改进或调整适应特殊场景。如果你需要进一步的改进或其他功能，请告诉我！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 你的LRU缓存算法实现得很好，代码简洁明了，逻辑清晰。以下是一些建议和改进点，可以帮助你进一步提升代码质量和可读性：\n",
       "\n",
       "### 代码改进建议：\n",
       "\n",
       "1. **类型注解**：\n",
       "   - 在`put`方法中，虽然你已经使用了类型注解，但可以考虑在`get`方法的返回值上也添加类型注解，以提高代码的可读性和可维护性。\n",
       "\n",
       "2. **异常处理**：\n",
       "   - 在`get`方法中，返回`-1`表示未找到，但可以考虑使用异常处理来更明确地表示错误情况，例如抛出`KeyError`。\n",
       "\n",
       "3. **文档字符串**：\n",
       "   - 为每个方法添加文档字符串（docstring），以便其他开发者（或未来的你）能快速理解每个方法的功能和用法。\n",
       "\n",
       "4. **性能优化**：\n",
       "   - 虽然`OrderedDict`在大多数情况下性能良好，但如果你需要在高并发环境下使用LRU缓存，可以考虑使用`threading.Lock`来确保线程安全。\n",
       "\n",
       "5. **测试用例**：\n",
       "   - 提供一些单元测试用例，以验证你的LRU缓存实现的正确性和边界情况。\n",
       "\n",
       "### 改进后的代码示例：\n",
       "\n",
       "```python\n",
       "from collections import OrderedDict\n",
       "\n",
       "class LRUCache:\n",
       "    def __init__(self, capacity: int):\n",
       "        \"\"\"\n",
       "        Initialize the LRUCache with a given capacity.\n",
       "        \"\"\"\n",
       "        self.cache = OrderedDict()\n",
       "        self.capacity = capacity\n",
       "\n",
       "    def get(self, key: int) -> int:\n",
       "        \"\"\"\n",
       "        Return the value of the key if the key exists in the cache, otherwise return -1.\n",
       "        Move the accessed item to the end to mark it as recently used.\n",
       "        \"\"\"\n",
       "        if key not in self.cache:\n",
       "            return -1\n",
       "        else:\n",
       "            self.cache.move_to_end(key)\n",
       "            return self.cache[key]\n",
       "\n",
       "    def put(self, key: int, value: int) -> None:\n",
       "        \"\"\"\n",
       "        Update the value of the key if the key exists, otherwise add the key-value pair to the cache.\n",
       "        If the cache exceeds its capacity, remove the least recently used item.\n",
       "        \"\"\"\n",
       "        if key in self.cache:\n",
       "            self.cache.move_to_end(key)\n",
       "        self.cache[key] = value\n",
       "        \n",
       "        if len(self.cache) > self.capacity:\n",
       "            self.cache.popitem(last=False)\n",
       "\n",
       "# 示例代码使用\n",
       "if __name__ == \"__main__\":\n",
       "    lru_cache = LRUCache(2)  # 创建一个容量为 2 的 LRUCache\n",
       "    lru_cache.put(1, 1)      # 缓存是 {1=1}\n",
       "    lru_cache.put(2, 2)      # 缓存是 {1=1, 2=2}\n",
       "    print(lru_cache.get(1))  # 返回 1\n",
       "    lru_cache.put(3, 3)      # 取消键 2，缓存是 {1=1, 3=3}\n",
       "    print(lru_cache.get(2))  # 返回 -1 (未找到)\n",
       "    lru_cache.put(4, 4)      # 取消键 1，缓存是 {3=3, 4=4}\n",
       "    print(lru_cache.get(1))  # 返回 -1 (未找到)\n",
       "    print(lru_cache.get(3))  # 返回 3\n",
       "    print(lru_cache.get(4))  # 返回 4\n",
       "```\n",
       "\n",
       "### 测试用例示例：\n",
       "\n",
       "```python\n",
       "def test_lru_cache():\n",
       "    lru_cache = LRUCache(2)\n",
       "    assert lru_cache.get(1) == -1  # 返回 -1 (未找到)\n",
       "    lru_cache.put(1, 1)\n",
       "    assert lru_cache.get(1) == 1   # 返回 1\n",
       "    lru_cache.put(2, 2)\n",
       "    assert lru_cache.get(2) == 2   # 返回 2\n",
       "    lru_cache.put(3, 3)             # 取消键 1\n",
       "    assert lru_cache.get(1) == -1   # 返回 -1 (未找到)\n",
       "    lru_cache.put(4, 4)             # 取消键 2\n",
       "    assert lru_cache.get(2) == -1   # 返回 -1 (未找到)\n",
       "    assert lru_cache.get(3) == 3     # 返回 3\n",
       "    assert lru_cache.get(4) == 4     # 返回 4\n",
       "\n",
       "test_lru_cache()\n",
       "```\n",
       "\n",
       "### 总结：\n",
       "通过以上改进，你的LRU缓存实现将更加健壮和易于维护。添加文档字符串和测试用例将有助于其他开发者理解和使用你的代码。希望这些建议对你有所帮助！如果你有其他问题或需要进一步的帮助，请随时告诉我。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 生成内容:\n",
       "- 非常感谢你的建议和反馈！你的改进意见非常有价值，这不仅有助于提升代码质量，还能让代码更加易于维护和扩展。以下是根据你的建议对代码进行的改进：\n",
       "\n",
       "### 主要改进点：\n",
       "1. **添加类型注解**：`get`方法的返回值也加上了类型注解，以增强可读性。\n",
       "2. **异常处理**：将`get`方法的返回值从`-1`改成抛出`KeyError`异常，这样能更明确地标示错误，并遵循Python的常规实践（比如字典操作中，找不到元素时抛出异常）。\n",
       "3. **文档字符串**：为类和方法添加了详细的文档字符串，方便理解每个方法的功能。\n",
       "4. **线程安全性**（可选）：虽然这个例子没有加入线程锁，但可以根据需要扩展成线程安全的版本。例如，可以在`put`和`get`方法中使用`threading.Lock`来确保多线程环境下的安全性。\n",
       "5. **测试用例**：为缓存类提供了一个简单的单元测试，用于验证LRU缓存的行为。\n",
       "\n",
       "### 改进后的代码：\n",
       "\n",
       "```python\n",
       "from collections import OrderedDict\n",
       "from typing import Optional\n",
       "\n",
       "class LRUCache:\n",
       "    def __init__(self, capacity: int):\n",
       "        \"\"\"\n",
       "        初始化LRU缓存实例，指定缓存的容量。\n",
       "        \n",
       "        :param capacity: 缓存的最大容量\n",
       "        \"\"\"\n",
       "        self.cache = OrderedDict()\n",
       "        self.capacity = capacity\n",
       "\n",
       "    def get(self, key: int) -> Optional[int]:\n",
       "        \"\"\"\n",
       "        获取缓存中指定key的值。如果key存在，则将该元素移动到缓存末尾（表示最近使用）。\n",
       "        如果key不存在，抛出KeyError。\n",
       "\n",
       "        :param key: 缓存中要获取的key\n",
       "        :return: 对应的值\n",
       "        :raises: KeyError 如果key不存在\n",
       "        \"\"\"\n",
       "        if key not in self.cache:\n",
       "            raise KeyError(f\"Key {key} not found.\")\n",
       "        # 将访问过的元素移动到末尾（表示最近使用）\n",
       "        self.cache.move_to_end(key)\n",
       "        return self.cache[key]\n",
       "\n",
       "    def put(self, key: int, value: int) -> None:\n",
       "        \"\"\"\n",
       "        将一个key-value对添加到缓存中。如果key已存在，则更新其值并将其移到末尾。\n",
       "        如果缓存已满，则移除最旧的（最久未使用的）元素。\n",
       "        \n",
       "        :param key: 要添加的key\n",
       "        :param value: 对应的值\n",
       "        \"\"\"\n",
       "        if key in self.cache:\n",
       "            self.cache.move_to_end(key)\n",
       "        self.cache[key] = value\n",
       "        \n",
       "        if len(self.cache) > self.capacity:\n",
       "            # 弹出最老的元素（即OrderedDict中的第一个元素）\n",
       "            self.cache.popitem(last=False)\n",
       "\n",
       "# 示例代码使用\n",
       "if __name__ == \"__main__\":\n",
       "    lru_cache = LRUCache(2)  # 创建一个容量为 2 的 LRUCache\n",
       "    lru_cache.put(1, 1)      # 缓存是 {1=1}\n",
       "    lru_cache.put(2, 2)      # 缓存是 {1=1, 2=2}\n",
       "    print(lru_cache.get(1))  # 返回 1\n",
       "    lru_cache.put(3, 3)      # 取消键 2，缓存是 {1=1, 3=3}\n",
       "    print(lru_cache.get(2))  # 返回 KeyError\n",
       "    lru_cache.put(4, 4)      # 取消键 1，缓存是 {3=3, 4=4}\n",
       "    print(lru_cache.get(1))  # 返回 KeyError\n",
       "    print(lru_cache.get(3))  # 返回 3\n",
       "    print(lru_cache.get(4))  # 返回 4\n",
       "```\n",
       "\n",
       "### 主要变动说明：\n",
       "\n",
       "1. **`get`方法**：当缓存中找不到`key`时，抛出`KeyError`异常，而不是返回`-1`。这更加符合Python的错误处理惯例，并且让异常处理更加明确。\n",
       "   \n",
       "   ```python\n",
       "   raise KeyError(f\"Key {key} not found.\")\n",
       "   ```\n",
       "\n",
       "2. **`put`方法**：保持原有逻辑，增加了文档字符串，解释每个步骤。\n",
       "\n",
       "3. **文档字符串**：每个方法和类都添加了简要的说明，帮助其他开发者更快理解功能。\n",
       "\n",
       "4. **线程安全**（可选）：如果在高并发场景下使用，可以为`put`和`get`方法添加线程锁，例如使用`threading.Lock`来保证线程安全。\n",
       "\n",
       "### 单元测试：\n",
       "\n",
       "```python\n",
       "def test_lru_cache():\n",
       "    lru_cache = LRUCache(2)\n",
       "    \n",
       "    # 测试get方法，初始时缓存为空，获取任何key都应该抛出异常\n",
       "    try:\n",
       "        lru_cache.get(1)\n",
       "    except KeyError as e:\n",
       "        assert str(e) == \"Key 1 not found.\"\n",
       "    \n",
       "    lru_cache.put(1, 1)\n",
       "    assert lru_cache.get(1) == 1   # 返回 1\n",
       "    \n",
       "    lru_cache.put(2, 2)\n",
       "    assert lru_cache.get(2) == 2   # 返回 2\n",
       "    \n",
       "    lru_cache.put(3, 3)             # 取消键 1\n",
       "    try:\n",
       "        lru_cache.get(1)\n",
       "    except KeyError as e:\n",
       "        assert str(e) == \"Key 1 not found.\"\n",
       "    \n",
       "    lru_cache.put(4, 4)             # 取消键 2\n",
       "    try:\n",
       "        lru_cache.get(2)\n",
       "    except KeyError as e:\n",
       "        assert str(e) == \"Key 2 not found.\"\n",
       "    \n",
       "    assert lru_cache.get(3) == 3     # 返回 3\n",
       "    assert lru_cache.get(4) == 4     # 返回 4\n",
       "\n",
       "test_lru_cache()\n",
       "```\n",
       "\n",
       "### 总结：\n",
       "\n",
       "通过这些改进，我们的LRU缓存实现更加健壮和易于理解，异常处理和文档字符串增强了代码的可维护性。单元测试可以帮助我们验证缓存逻辑是否正确，并确保边界情况得到了良好的处理。希望这些改进对你有帮助，如果有更多问题或改进点，随时告诉我！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 非常高兴看到你根据我的建议对代码进行了改进！你的更新使得LRU缓存的实现更加符合Python的最佳实践，并且增强了代码的可读性和可维护性。以下是一些额外的建议和观察，帮助你进一步提升代码质量：\n",
       "\n",
       "### 进一步的改进建议：\n",
       "\n",
       "1. **异常处理的灵活性**：\n",
       "   - 你可以考虑在`get`方法中使用自定义异常类，而不是直接抛出`KeyError`。这样可以提供更具体的错误信息，并且在大型项目中更易于管理。\n",
       "\n",
       "   ```python\n",
       "   class CacheMissError(Exception):\n",
       "       pass\n",
       "\n",
       "   def get(self, key: int) -> Optional[int]:\n",
       "       if key not in self.cache:\n",
       "           raise CacheMissError(f\"Key {key} not found.\")\n",
       "       self.cache.move_to_end(key)\n",
       "       return self.cache[key]\n",
       "   ```\n",
       "\n",
       "2. **缓存容量的动态调整**：\n",
       "   - 如果需要，可以考虑实现动态调整缓存容量的功能。例如，允许在运行时增加或减少缓存的容量，并相应地处理缓存中的数据。\n",
       "\n",
       "3. **增加缓存统计信息**：\n",
       "   - 可以添加一些统计信息，比如命中率、总访问次数等，以便更好地监控缓存的性能。\n",
       "\n",
       "4. **线程安全性**：\n",
       "   - 如果你决定在多线程环境中使用LRU缓存，确保在`put`和`get`方法中使用`threading.Lock`来保护对共享数据的访问。\n",
       "\n",
       "   ```python\n",
       "   import threading\n",
       "\n",
       "   class LRUCache:\n",
       "       def __init__(self, capacity: int):\n",
       "           self.cache = OrderedDict()\n",
       "           self.capacity = capacity\n",
       "           self.lock = threading.Lock()\n",
       "\n",
       "       def get(self, key: int) -> Optional[int]:\n",
       "           with self.lock:\n",
       "               # 其余代码保持不变\n",
       "               ...\n",
       "       \n",
       "       def put(self, key: int, value: int) -> None:\n",
       "           with self.lock:\n",
       "               # 其余代码保持不变\n",
       "               ...\n",
       "   ```\n",
       "\n",
       "5. **性能测试**：\n",
       "   - 考虑编写一些性能测试，尤其是在高负载情况下，确保LRU缓存的性能符合预期。\n",
       "\n",
       "### 总结：\n",
       "\n",
       "你的代码已经非常出色，以上建议只是为了进一步增强其功能和灵活性。通过引入自定义异常、动态容量调整、缓存统计信息等功能，你可以使LRU缓存更加健壮和适应不同的使用场景。\n",
       "\n",
       "如果你有其他问题或需要进一步的帮助，请随时告诉我！继续保持良好的编程习惯，祝你在编程的旅程中取得更多的成功！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 生成内容:\n",
       "- 非常感谢你的进一步反馈和建议！您提出的思路非常实用，可以进一步增强LRU缓存的灵活性与可维护性。以下是根据您的建议进行了改进后的代码实现：\n",
       "\n",
       "### 主要改进点：\n",
       "1. **自定义异常**：新建一个自定义异常类`CacheMissError`来表示缓存未命中，有助于让代码更具可读性和管理性。\n",
       "2. **动态容量调整**（可选）：增加`resize`方法，用于动态调整缓存的容量，并处理超出新容量的元素。\n",
       "3. **缓存统计信息**（可选）：添加命中率和访问统计功能，允许查询缓存的基本性能指标。\n",
       "4. **线程安全**：使用`threading.Lock`来确保多线程访问的安全性。\n",
       "\n",
       "### 改进后的代码实现：\n",
       "\n",
       "```python\n",
       "from collections import OrderedDict\n",
       "from typing import Optional\n",
       "\n",
       "class CacheMissError(Exception):\n",
       "    \"\"\"自定义异常表示缓存未命中\"\"\"\n",
       "    pass\n",
       "\n",
       "class LRUCache:\n",
       "    def __init__(self, capacity: int):\n",
       "        \"\"\"\n",
       "        初始化LRUCache实例，指定缓存的容量。\n",
       "\n",
       "        :param capacity: 缓存的最大容量\n",
       "        \"\"\"\n",
       "        self.cache = OrderedDict()\n",
       "        self.capacity = capacity\n",
       "        self.lock = threading.Lock()\n",
       "        self.hits = 0\n",
       "        self.misses = 0\n",
       "\n",
       "    def get(self, key: int) -> Optional[int]:\n",
       "        \"\"\"\n",
       "        获取缓存中指定key的值。 \n",
       "        如果key不存在，抛出CacheMissError。\n",
       "        \n",
       "        :param key: 缓存中要获取的key\n",
       "        :return: 对应的值\n",
       "        :raises: CacheMissError 如果key不存在\n",
       "        \"\"\"\n",
       "        with self.lock:\n",
       "            if key not in self.cache:\n",
       "                self.misses += 1\n",
       "                raise CacheMissError(f\"Key {key} not found.\")\n",
       "            self.hits += 1\n",
       "            self.cache.move_to_end(key)\n",
       "            return self.cache[key]\n",
       "\n",
       "    def put(self, key: int, value: int) -> None:\n",
       "        \"\"\"\n",
       "        更新或添加一个key-value对到缓存。\n",
       "        如果缓存已满，则移除最旧的元素。\n",
       "\n",
       "        :param key: 要添加的key\n",
       "        :param value: 对应的值\n",
       "        \"\"\"\n",
       "        with self.lock:\n",
       "            if key in self.cache:\n",
       "                self.cache.move_to_end(key)\n",
       "            self.cache[key] = value\n",
       "            \n",
       "            if len(self.cache) > self.capacity:\n",
       "                self.cache.popitem(last=False)\n",
       "\n",
       "    def resize(self, new_capacity: int) -> None:\n",
       "        \"\"\"\n",
       "        动态调整缓存容量。如果减小容量，移除未使用的元素。\n",
       "\n",
       "        :param new_capacity: 新的缓存容量\n",
       "        \"\"\"\n",
       "        with self.lock:\n",
       "            self.capacity = new_capacity\n",
       "            while len(self.cache) > self.capacity:\n",
       "                self.cache.popitem(last=False)\n",
       "\n",
       "    def get_statistics(self) -> dict:\n",
       "        \"\"\"\n",
       "        获取缓存命中率和访问统计信息。\n",
       "\n",
       "        :return: 包含命中次数和未命中次数的字典\n",
       "        \"\"\"\n",
       "        total_requests = self.hits + self.misses\n",
       "        hit_rate = (self.hits / total_requests) * 100 if total_requests > 0 else 0\n",
       "        return {'hits': self.hits, 'misses': self.misses, 'hit_rate': hit_rate}\n",
       "\n",
       "# 示例代码使用\n",
       "if __name__ == \"__main__\":\n",
       "    import threading\n",
       "\n",
       "    lru_cache = LRUCache(2)  # 创建一个容量为 2 的 LRUCache\n",
       "    lru_cache.put(1, 1)\n",
       "    lru_cache.put(2, 2)\n",
       "\n",
       "    try:\n",
       "        print(lru_cache.get(1))  # 返回 1\n",
       "        lru_cache.put(3, 3)      # 取消键 2\n",
       "        lru_cache.get(2)         # 返回 CacheMissError\n",
       "    except CacheMissError as e:\n",
       "        print(e)\n",
       "\n",
       "    lru_cache.put(4, 4)          # 取消键 1\n",
       "    try:\n",
       "        print(lru_cache.get(1))  # 返回 CacheMissError\n",
       "    except CacheMissError as e:\n",
       "        print(e)\n",
       "        \n",
       "    # 获取缓存统计信息\n",
       "    stats = lru_cache.get_statistics()\n",
       "    print(stats)  # 输出命中率、命中次数和未命中次数\n",
       "```\n",
       "\n",
       "### 改进后功能说明：\n",
       "\n",
       "1. **自定义异常** `CacheMissError`：增强了对未命中情况的处理，让代码的意图更加明确。\n",
       "  \n",
       "2. **动态容量调整**：实现了`resize`方法，可以通过调整缓存容量来动态管理缓存。若缓存容量缩小，最不常用的元素会被移除。\n",
       "   \n",
       "3. **缓存统计功能**：添加`get_statistics`方法返回命中率、命中和未命中次数，方便监控缓存性能特色。\n",
       "\n",
       "4. **线程安全**：加上`threading.Lock()`确保在多线程情况下对缓存的安全访问。\n",
       "\n",
       "### 性能测试：\n",
       "\n",
       "考虑编写一些性能测试，例如通过多线程并发访问来测试缓存的线程安全性及其表现，确保在高访问量的场景下仍能良好运作。\n",
       "\n",
       "希望这些改进和建议对您有帮助！如果还有更多构想或问题，请继续提问，我很乐意协助！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 你的改进非常全面，已经涵盖了线程安全、缓存统计、动态容量调整等重要功能，并且采用了自定义异常，使得代码更加健壮和可维护。这些改进不仅增强了LRU缓存的灵活性，还提升了代码的可读性和扩展性。\n",
       "\n",
       "### 进一步的优化与建议：\n",
       "\n",
       "1. **线程安全的性能考量**：\n",
       "   - 在高并发环境下，使用`threading.Lock`可以保证线程安全，但每次访问缓存都需要获取锁，这可能会影响性能。若缓存访问量较高，或者缓存操作是频繁的，可以考虑更细粒度的锁机制或使用`threading.RLock`来避免死锁。\n",
       "   \n",
       "   ```python\n",
       "   self.lock = threading.RLock()  # 递归锁，支持同一线程多次获取\n",
       "   ```\n",
       "\n",
       "2. **缓存优化**：\n",
       "   - 如果希望进一步提高LRU缓存的效率，可以考虑将`OrderedDict`替换为一个更加高效的结构，如双向链表配合哈希表，来避免频繁的`move_to_end`操作。`OrderedDict`在大部分情况下性能很好，但`move_to_end`操作的时间复杂度为O(1)，但是在高频率的读写操作中，可能仍然会有性能瓶颈。\n",
       "\n",
       "3. **内存管理和限制**：\n",
       "   - 如果LRU缓存存储的数据非常大，可以考虑加入内存限制机制。当内存占用达到一定阈值时，主动清理掉部分缓存数据。这对于大规模缓存尤为重要，可以避免缓存占用过多内存。\n",
       "\n",
       "4. **增加缓存清理功能**：\n",
       "   - 虽然LRU算法会自动移除最久未使用的元素，但你可以考虑提供一个手动清理缓存的功能，尤其是在应用程序需要释放内存或清理旧数据的情况下。\n",
       "   \n",
       "   ```python\n",
       "   def clear_cache(self):\n",
       "       with self.lock:\n",
       "           self.cache.clear()\n",
       "   ```\n",
       "\n",
       "5. **测试与性能分析**：\n",
       "   - 你可以进一步进行性能分析，尤其是模拟高并发场景下LRU缓存的表现。比如，模拟并发访问缓存的线程数，测试缓存的响应时间和命中率，分析它在不同压力下的表现。\n",
       "\n",
       "### 示例：高并发下的性能测试\n",
       "\n",
       "如果你想测试在多线程环境下的LRU缓存，可以使用以下简单的多线程并发访问测试：\n",
       "\n",
       "```python\n",
       "import threading\n",
       "import time\n",
       "\n",
       "def test_cache_concurrently(lru_cache):\n",
       "    def worker(key, value):\n",
       "        try:\n",
       "            lru_cache.put(key, value)\n",
       "            print(f\"Put {key}={value}\")\n",
       "            print(f\"Get {key}={lru_cache.get(key)}\")\n",
       "        except CacheMissError as e:\n",
       "            print(e)\n",
       "    \n",
       "    threads = []\n",
       "    for i in range(100):\n",
       "        # 启动多个线程访问缓存\n",
       "        t = threading.Thread(target=worker, args=(i, i * 10))\n",
       "        threads.append(t)\n",
       "        t.start()\n",
       "    \n",
       "    for t in threads:\n",
       "        t.join()\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    lru_cache = LRUCache(5)  # 容量为5的LRU缓存\n",
       "    test_cache_concurrently(lru_cache)\n",
       "```\n",
       "\n",
       "这段代码会启动多个线程并行访问LRU缓存，从而测试在并发高负载下缓存的响应能力和线程安全性。\n",
       "\n",
       "### 总结\n",
       "\n",
       "你的LRU缓存实现已经非常完备，且考虑到了多线程环境下的安全性、灵活的容量调整和缓存性能监控。通过上述进一步的优化建议，你可以根据不同的需求扩展缓存功能，或提升缓存性能。如果你计划将LRU缓存投入生产环境，考虑通过性能分析来确保它能够在高负载的情况下表现良好。\n",
       "\n",
       "如果你有任何进一步的问题或想法，随时向我提出，我非常乐意继续为你提供帮助！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 生成内容:\n",
       "- 非常感谢您的详细反馈和建议，这些都是非常有价值的改进方向！以下是根据您的建议进一步优化LRU缓存代码的一些思考，以及可以实施的方法。\n",
       "\n",
       "### 进一步优化LRU静态缓存的实现和调整：\n",
       "\n",
       "1. **采用递归锁**：\n",
       "   我们可以使用`threading.RLock`替代`threading.Lock`，后者适合在同一线程中可重入，避免了死锁情况。但需注意，加锁会影响性能，应当在性能需求严格的情况下谨慎使用。\n",
       "\n",
       "2. **改进缓存存储的结构**：\n",
       "   考虑到性能问题，可以用一个字典结合双向链表实现LRU缓存，以提高将刚使用的项放到末尾的效率。如下构想中的双向链表节点类。\n",
       "\n",
       "   ```python\n",
       "   class Node:\n",
       "       def __init__(self, key, value):\n",
       "           self.key = key\n",
       "           self.value = value\n",
       "           self.prev = None\n",
       "           self.next = None\n",
       "   ```\n",
       "\n",
       "   可以更浅显改善其O(1)复杂性的查找效率。\n",
       "\n",
       "3. **内存限制机制**：\n",
       "   有必要引入内存监控，及时调整数据项的数量，防止JavaGB利用过高。此外，若需求额外更新缓存，允许手动或定期清理缓存变得可用。\n",
       "\n",
       "4. **增设手动清理功能**：\n",
       "   提供手动清理比较旧缓存项的能力，清理特定主动使用下使得应用更佳灵活。\n",
       "\n",
       "   ```python\n",
       "   def clear_cache(self) -> None:\n",
       "       with self.lock:\n",
       "           self.cache.clear()\n",
       "           self.hits = 0\n",
       "           self.misses = 0\n",
       "   ```\n",
       "\n",
       "5. **并发性能分析**：\n",
       "   可以依据多线程高并发示例，上述为挖掘响应能力，带来具体瓶颈测试。\n",
       "\n",
       "### 示例改良代码：\n",
       "\n",
       "以下是改进后的LRU缓存实现，结合多线措施和给定的自定义要求：\n",
       "\n",
       "```python\n",
       "from collections import OrderedDict\n",
       "from typing import Optional\n",
       "import threading\n",
       "\n",
       "class CacheMissError(Exception):\n",
       "    \"\"\"自定义异常表示缓存未命中\"\"\"\n",
       "    pass\n",
       "\n",
       "class Node:\n",
       "    \"\"\"双向链表节点类\"\"\"\n",
       "    def __init__(self, key, value):\n",
       "        self.key = key\n",
       "        self.value = value\n",
       "        self.prev = None\n",
       "        self.next = None\n",
       "\n",
       "class LRUCache:\n",
       "    def __init__(self, capacity: int):\n",
       "        self.cache = {}\n",
       "        self.capacity = capacity\n",
       "        self.head = Node(0, 0)\n",
       "        self.tail = Node(0, 0)\n",
       "        self.head.next = self.tail\n",
       "        self.tail.prev = self.head\n",
       "        self.lock = threading.RLock()  # 使用重入锁\n",
       "        self.hits = 0\n",
       "        self.misses = 0\n",
       "\n",
       "    def _add_node(self, node: Node) -> None:\n",
       "        \"\"\"在尾部添加节点\"\"\"\n",
       "        node.prev = self.tail.prev\n",
       "        node.next = self.tail\n",
       "        self.tail.prev.next = node\n",
       "        self.tail.prev = node\n",
       "\n",
       "    def _remove_node(self, node: Node) -> None:\n",
       "        \"\"\"移除节点\"\"\"\n",
       "        prev_node, next_node = node.prev, node.next\n",
       "        prev_node.next = next_node\n",
       "        next_node.prev = prev_node\n",
       "\n",
       "    def get(self, key: int) -> Optional[int]:\n",
       "        with self.lock:\n",
       "            if key not in self.cache:\n",
       "                self.misses += 1\n",
       "                raise CacheMissError(f\"Key {key} not found.\")\n",
       "            node = self.cache[key]\n",
       "            self._remove_node(node)\n",
       "            self._add_node(node)\n",
       "            self.hits += 1\n",
       "            return node.value\n",
       "\n",
       "    def put(self, key: int, value: int) -> None:\n",
       "        with self.lock:\n",
       "            if key in self.cache:\n",
       "                self._remove_node(self.cache[key])\n",
       "            new_node = Node(key, value)\n",
       "            self._add_node(new_node)\n",
       "            self.cache[key] = new_node\n",
       "            \n",
       "            if len(self.cache) > self.capacity:\n",
       "                lru_node = self.head.next\n",
       "                self._remove_node(lru_node)\n",
       "                del self.cache[lru_node.key]\n",
       "\n",
       "    def clear_cache(self) -> None:\n",
       "        with self.lock:\n",
       "            self.cache.clear()\n",
       "            self.hits = 0\n",
       "            self.misses = 0\n",
       "\n",
       "    def get_statistics(self) -> dict:\n",
       "        total_requests = self.hits + self.misses\n",
       "        hit_rate = (self.hits / total_requests) * 100 if total_requests > 0 else 0\n",
       "        return {'hits': self.hits, 'misses': self.misses, 'hit_rate': hit_rate}\n",
       "```\n",
       "\n",
       "### 高并发性能测试示例\n",
       "\n",
       "接下来，您可以使用以下代码在高并发场景中测试LRU缓存的表现：\n",
       "\n",
       "```python\n",
       "import threading\n",
       "\n",
       "def test_cache_concurrently(lru_cache):\n",
       "    def worker(key, value):\n",
       "        try:\n",
       "            lru_cache.put(key, value)\n",
       "            print(f\"Put {key}={value}: {lru_cache.get(key)}\")\n",
       "        except CacheMissError as e:\n",
       "            print(e)\n",
       "    \n",
       "    threads = []\n",
       "    for i in range(10):\n",
       "        # 启动多个线程，并发写入和读取\n",
       "        t = threading.Thread(target=worker, args=(i, i * 100))\n",
       "        threads.append(t)\n",
       "        t.start()\n",
       "    \n",
       "    for t in threads:\n",
       "        t.join()\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    lru_cache = LRUCache(5)  # 容量为5的LRU缓存\n",
       "    test_cache_concurrently(lru_cache)\n",
       "```\n",
       "\n",
       "上述示例能够有效地以一定量的线程并行访缓存，来测炼并存缓存的性能以及检测线程安全问题。\n",
       "\n",
       "### 总结\n",
       "\n",
       "通过不断的改进和讨论，您的LRU缓存实现已经非常全面，并能在实际开发环境中发挥作用。优化建议内容包括合适的线程抢占机制、内存限制机制、节点 ورځ引理、测试能兼顾多线程的样式等。\n",
       "\n",
       "如果您还有其他相关问题或请求关于特定方面的帮助，欢迎随时与我沟通。继续祝贺您在这次实践中取得的成绩，同时期待对您以后开发路上的支持！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"用python实现LRU缓存算法\")\n",
    "    ],\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(inputs, config):\n",
    "    pretty_print_event_markdown(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9e339-eb87-4707-a019-8387faaa52bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44173cab-4e51-4bd7-8c97-a28748e159a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f693e-441e-4536-8459-8042b34e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6092e5ee-7263-4f62-818a-0cc33ca0b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"参考西游记唐僧的说话风格，写一篇劝年轻人结婚买房的文章\")\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# async for event in graph.astream(inputs, config):\n",
    "#     pretty_print_event_markdown(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e9c24-7cf7-42fa-be13-fbfebfdad5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a5187b-6b3b-4266-a226-f1e56e5ad350",
   "metadata": {},
   "source": [
    "## Homework: \n",
    "\n",
    "1. 扩展本指南的 Reflection Agent，使其能够完成更通用的生成任务，包括但不限于代码、报告等；\n",
    "2. 使用扩展后的 Reflection Agent 生成代码，实现在 GitHubSentinel 上新增一个信息渠道。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae364f4-1792-4f13-bc25-1e20a4547961",
   "metadata": {},
   "source": [
    "# 作业总结：\n",
    "完成Reflection Agent作业，双方都是gpt-4o-mini，以下是我修改了提示词并看完对话结果后的总结：\n",
    "\n",
    "- 提问：用python实现LRU缓存算法\n",
    "- Round 1：程序员用OrderedDict实现LRU缓存，支持按顺序添加和删除。\n",
    "- Round 2：评审员提出了添加类型注解、异常处理、多线程保护和测试用例的建议。\n",
    "- Round 3：程序员做了改进，但是没有实现多线程保护，希望调用方来加锁。\n",
    "- Round 4：评审员一看没有实现多线程保护，于是提供一个内部实现加锁的模板（让你就照着这么写），以及要求加个高负载的性能测试。\n",
    "- Round 5：程序员实现了多线程方案，以及被要求的动态调整内存容量，统计信息，但测试依旧很简单（没有大改）。\n",
    "- Round 6：评审员提出用递归锁来避免死锁，以及用双向链表+哈希表换掉OrderedDict以增加性能，并且提供了支持并发的测试案例（依然让你照着写）\n",
    "- Round 7：程序员按要求重构了实现和测试用例\n",
    "\n",
    "不得不说这评审员还是挺严格的，会检查你有没有完成我提过的每个要求，如果没有完成就觉得你大概是不明白怎么做，然后再提供一个示例让你照着改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affbb4d-f154-43e6-ae3d-d6a5dcd6e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f9e0b07-62a8-430b-bf01-b7d8c07dd6e5",
   "metadata": {},
   "source": [
    "### 如何让 Reflection `System Prompt` 更加通用：\n",
    "\n",
    "如果你想让这个 `System Prompt` 适用于更广泛的内容评估场景，不局限于作文，你可以做一些轻微的调整。例如：\n",
    "\n",
    "```python\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a reviewer tasked with providing constructive critique and improvement suggestions for the user's submission.\"\n",
    "            \" Offer detailed feedback, including recommendations on clarity, structure, content depth, and style, as well as areas for improvement.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### 修改后的变化：\n",
    "1. **角色定位更广泛**：从“老师”改为“审阅者”，这样不局限于评估作文，适用于各种类型的内容，包括文章、报告、甚至代码审查。\n",
    "  \n",
    "2. **批评与改进建议的灵活性**：从作文的“长度、深度、风格”拓展为“清晰度、结构、内容深度、风格”，这使得反馈更加多样化，适用于不同的内容类型。\n",
    "\n",
    "通过这种方式，可以让模型在更多场景下提供高质量的评估和反馈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b78d64-791c-484c-922d-d00a01b78a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
